plot(airquality$Wind, airquality$Ozone, main = "Ozone and Wind")
plot(airquality$Solar.R, airquality$Ozone, main = "Ozone and Solar Radiation")
plot(airquality$Temp, airquality$Ozone, main "Ozone and Temperature")
plot(airquality$Temp, airquality$Ozone, main = "Ozone and Temperature")
mtext("Ozone and Weather in New York City", outer = TRUE)
library(swirl)
swirl()
mtext("Ozone and Weather in New York City", outer = TRUE)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
mtext("Ozone and Weather in New York City", outer = TRUE)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
myxlab <- "Carat"
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
library(swirl)
library(swirl)
swirl()
swirl()
head(airquality)
xyplot(Ozone~Wind, data=airquality)
xyplot(Ozone~Wind, data=airquality, col="red", pch=8, main="Big Apple Data")
xyplot(Ozone~Wind, data=airquality, pch=8, col="red", main="Big Apple Data")
xyplot(Ozone~Wind | as.factor(Month), data=airquality, layout=c(5,1))
xyplot(Ozone~Wind | Month, data=airquality, layout=c(5,1))
p <- xyplot(Ozone~Wind, data=airquality)
p
names(p)
mynames[myfull]
p[["formula"]]
p[[x.limits]]
p[["x.limits"]]
table(f)
xyplot(y~x| x, layout=c(2,1))
xyplot(y ~x | f, layout=c(2, 1))
v1
v2
myedit("plot1.R")
source(pathtofile("plot1.R", local=TRUE))
source(pathtofile("plot1.R", local==TRUE))
source(pathtofile("plot1.R"), local=TRUE)
myedit("plot2.R")
source(pathtofile("plot2.R"), local=TRUE)
str(diamonds)
table(diamonds$color)
table(diamonds$color, diamonds$cut)
myedit("myLabels.R")
pathtofile("myLabels.R")
source(pathtofile("myLabels.R"), local=TRUE)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
xyplot(price~carat|color*cut,data=diamonds,strip=FALSE,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
sample(colors(), 10)
pal <- colorRamp(c("red", "blue"))
pal(0)
pal(1)
pal(seq(0,1,len=6))
p1 <- colorRampPalette(c("red", "blue"))
p1
p1(2)
p1(6)
0xcc
colorRampPalette(c("red","yellow")
)
p2 <- colorRampPalette(c("red", "yellow"))
p2
p2(2)
p2(10)
p1(20)
showMe(p1(20))
showme(p2(20))
showMe(p2(20))
showMe(p2(2))
p1
?fun
?rgb
p3 <- colorRampPalette((c"blue", "green"), alpha=.5)
p3 <- colorRampPalette(c("blue", "green"), alpha=.5)
p3(5)
plot(x, y, pch=19, col=rgb(0, .5, .5)
)
plot(x, y, pch=19, col=rgb(0, .5, .5, .3)
)
cols(brewer.pal(3, "BuGn"))
cols <- (brewer.pal(3, "BuGn"))
cols <- brewer.pal(3, "BuGn")
showMe(cols)
pal <- colorRampPalette(cols)
showMe(pal(20))
image(volcano, col=pal(20))
image(volcano, col=p1(20))
str(mpg)
qplot(displ, hwy, data=mpg)
qplot(displ, hwy, data=mpg, col=drv)
qplot(displ, hwy, data=mpg, color=drv)
qplot(displ, hwy, data=mpg, color=drv, geom=c("point", "smooth"))
qplot(y=hmy, data=mpg, color=drv)
qplot(y=hmy, data=mpg, color=drv)
qplot(y=hwy, data = mpg, color = drv)
myhigh
qplot(drv,hwy,data=mpg,geom="boxplot")
qplot(drv, hwy, data=mpg, geom="boxplot", color = manufacturer)
qplor(hwy, data=mpg, fill=drv)
qplot(hwy, data=mpg, fill=drv)
qplot(displ, hwy, data=mgp, facets = .~drv)
qplot(displ, hwy, data = mgp, facets = .~drv)
qplot(displ, hwy, data = mpg, facets = . ~ drv)
qplot(hwy, data = mpg, facets = drv~ ., binwidth = 2)
## Load Data
data <- read.csv("activity.csv")
## Load Data
data <- read.csv("activity.csv")
## Load Data
data <- read.csv("activity.csv")
library(swift)
library(swirl)
enrique1790
swirl()
swirl()
ngramTokenizer <- function(l) {
function(x) unlist(lapply(ngrams(words(x), l), paste, collapse = " "), use.names = FALSE)
}
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_unigram <- generateNgramData(1)
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
corpus_stats <- "D:/Coursera/Coursera-SwiftKey/final/en_US/corpus_stats.Rda"
corpus_stats <- data.frame(""D:/Coursera/Coursera-SwiftKey/final/en_US/corpus_stats.Rda")")
library(tm)
library(RWekajars)
library(RWeka)
library(wordcloud)
require(openNLP)
require(reshape)
set.seed(892)
sample_pct <- .4
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines("en_US.blogs.txt", skipNul = T))
setwd("D:/Coursera/Coursera-SwiftKey/final/en_US")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines("en_US.blogs.txt", skipNul = T))
news_data <- tolower(readLines("en_US.news.txt", skipNul = T))
twitter_data <- tolower(readLines("en_US.twitter.txt", skipNul = T))
library(tm)
bigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm.Rda")
trigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_trigram_sm.Rda")
fourgram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_fourgram_sm.Rda")
nextWordPredictor <- function(inputTxt) {
if(nchar(inputTxt) > 0) {
#clean input
inputTxt <- tolower(inputTxt)
inputTxt <- removeNumbers(inputTxt)
inputTxt <- removePunctuation(inputTxt)
inputTxt <- stripWhitespace(inputTxt)
#split into words
inputList <- unlist(strsplit(inputTxt, " "))
print(inputList)
numWords <- length(inputList)
print(numWords)
runBigram <- function(words){
bigram[bigram$terms$one == words,]$terms$two
}
runTrigram <- function(words){
trigram[trigram$terms$one == words[1] &
trigram$terms$two == words[2],]$terms$three
}
runFourgram <- function(words) {
fourgram[ fourgram$terms$one == words[1] &
fourgram$terms$two == words[2] &
fourgram$terms$three == words[3],]$terms$four
}
if(numWords == 1) {
#print("running bigram")
predList <- runBigram(inputList[1])
}else if (numWords == 2) {
#print("running trigram")
word1 <- inputList[1]
word2 <- inputList[2]
predList <- runTrigram(c(word1, word2))
if(length(predList) == 0){
#print("Trigram failed running bigram")
predList <- runBigram(word2)
}
}else {
#print("running fourgram")
word1 <- inputList[numWords-2]
word2 <- inputList[numWords-1]
word3 <- inputList[numWords]
predList <- runFourgram(c(word1, word2, word3))
if(length(predList) == 0){
#print("fourgram failed running trigram")
predList <- runTrigram(c(word2,word3))
}
if(length(predList) == 0){
#print("trigram failed running bigram")
predList <- runBigram(word3)
}
}
#Return top n predictors
n <- 4
tp <- length(predList)
if( tp >= n){
predList <- predList[1:n]
}
as.character(predList)
}else{
""
}
}
nextWordPredictor("my name is")
bad_words_data <- readLines("full-list-of-bad-words.txt", skipNul = T)
#search for specific word ratios
twitter_num_love <- sum(grepl("love", twitter_data) == TRUE)
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines("en_US.blogs.txt", skipNul = T))
#Get files locations
us_txt_dir <- "final/en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
#Get files locations
us_txt_dir <- setwd("D:/Coursera/Coursera-SwiftKey/final/en_US")
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
install.packages(reshape)
install.packages("reshape")
install.packages("reshape")
install.packages("openNLP")
library(tm)
library(RWekajars)
library(RWeka)
library(wordcloud)
require(openNLP)
require(reshape)
set.seed(892)
sample_pct <- .4
#Get files locations
us_txt_dir <- setwd("D:/Coursera/Coursera-SwiftKey/final/en_US")
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Source http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/
bad_words_txt <- paste(us_txt_dir, "full-list-of-bad-words.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_data <- tolower(readLines(news_txt, skipNul = T))
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines("en_US.blogs.txt", skipNul = T))
set.seed(892)
sample_pct <- .4
news_data <- tolower(readLines("en_US.news.txt", skipNul = T))
twitter_data <- tolower(readLines("en_US.twitter.txt", skipNul = T))
bad_words_data <- readLines("full-list-of-bad-words.txt", skipNul = T)
blogs_size <- round(file.size("en_US.blogs.txt")/1048576, 2)
news_size <- round(file.size("en_US.news.txt")/1048576, 2)
twitter_size <- round(file.size("en_US.twitter.txt")/1048576, 2)
#Get line counts
blogs_lines <- length(blogs_data)
news_lines <- length(news_data)
twitter_lines <- length(twitter_data)
#Get max line length
blogs_char_cnt <- lapply(blogs_data, nchar)
blogs_max_chars <- blogs_char_cnt[[which.max(blogs_char_cnt)]]
news_char_cnt <- lapply(news_data, nchar)
news_max_chars <- news_char_cnt[[which.max(news_char_cnt)]]
twitter_char_cnt <- lapply(twitter_data, nchar)
twitter_max_chars <- twitter_char_cnt[[which.max(twitter_char_cnt)]]
#Get word counts (based on spaces)
blogs_words <- sum( sapply(gregexpr("\\S+", blogs_data), length ) )
news_words <- sum( sapply(gregexpr("\\S+", news_data), length ) )
twitter_words <- sum( sapply(gregexpr("\\S+", twitter_data), length ) )
#Summary of corpus stats
corpus_stats <- data.frame( "Files" = c("Blogs", "News", "Twitter"),
"Lines" = c(blogs_lines, news_lines, twitter_lines),
"Longest_Line" = c(blogs_max_chars, news_max_chars, twitter_max_chars),
"Words" = c(blogs_words, news_words, twitter_words),
"File_Size_Mb" = c(blogs_size, news_size, twitter_size))
#search for specific word ratios
twitter_num_love <- sum(grepl("love", twitter_data) == TRUE)
twitter_hate_num <- sum(grepl("hate", twitter_data) == TRUE)
twitter_love_hate_ratio <- twitter_num_love / twitter_hate_num
#search specific words in a sentence
bs_line <- twitter_data[grepl("biostats", twitter_data)]
chess_line_cnt <- sum(grepl("A computer once beat me at chess, but it was no match for me at kickboxing", twitter_data, ignore.case = T) == TRUE)
####################### Analyze Blogs Data
blogs_data_sample <- blogs_data[sample(1:blogs_lines, blogs_lines*sample_pct)]
blogs_cp <- Corpus(VectorSource(list(blogs_data_sample)))
#Clean up corpus
blogs_cp <- tm_map(blogs_cp, removeNumbers)
blogs_cp <- tm_map(blogs_cp, removePunctuation)
blogs_cp <- tm_map(blogs_cp, stripWhitespace)
#Create doc term matrix
blogs_dtm <- DocumentTermMatrix(blogs_cp, control = list(stopwords = TRUE))
#Find frequent words
blogs_dtm_mtrx <- as.matrix(blogs_dtm)
blogs_frequency <- colSums(blogs_dtm_mtrx)
blogs_frequency <- sort(blogs_frequency, decreasing = TRUE)
######################## Analyze News Data
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
news_cp <- Corpus(VectorSource(list(news_data_sample)))
#Clean up corpus
news_cp <- tm_map(news_cp, removeNumbers)
news_cp <- tm_map(news_cp, removePunctuation)
news_cp <- tm_map(news_cp, removeWords, stopwords('english'))
news_cp <- tm_map(news_cp, stripWhitespace)
#Create doc term matrix
news_dtm <- DocumentTermMatrix(news_cp)
#Find frequent words
news_dtm_mtrx <- as.matrix(news_dtm)
news_frequency <- colSums(news_dtm_mtrx)
news_frequency <- sort(news_frequency, decreasing = TRUE)
######################## Analyze Twitter Data
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
twitter_cp <- Corpus(VectorSource(list(twitter_data_sample)))
#Clean up corpus
twitter_cp <- tm_map(twitter_cp, removeNumbers)
twitter_cp <- tm_map(twitter_cp, removePunctuation)
twitter_cp <- tm_map(twitter_cp, removeWords, stopwords('english'))
twitter_cp <- tm_map(twitter_cp, stripWhitespace)
#Create doc term matrix
twitter_dtm <- DocumentTermMatrix(twitter_cp)
#Find frequent words
twitter_dtm_mtrx <- as.matrix(twitter_dtm)
twitter_frequency <- colSums(twitter_dtm_mtrx)
twitter_frequency <- sort(twitter_frequency, decreasing = TRUE)
######################## Analyze Full Data
#Create smaller samples for further processing of combined corpus
sample_pct <- .1
blogs_data_sample <- blogs_data[sample(1:blogs_lines, blogs_lines*sample_pct)]
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
sample_data <- list(blogs_data_sample, news_data_sample, twitter_data_sample)
#Create Corpus
cp <- Corpus(VectorSource(sample_data))
#Clean up corpus
cp <- tm_map(cp, removeWords, bad_words_data )
cp <- tm_map(cp, removeNumbers)
cp <- tm_map(cp, removePunctuation)
cp <- tm_map(cp, stripWhitespace)
#Create doc term matrix
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
#Find frequent words
dtm_mtrx <- as.matrix(dtm)
frequency <- colSums(dtm_mtrx)
frequency <- sort(frequency, decreasing = TRUE)
wordcloud(names(frequency), frequency, min.freq = 25, random.order = FALSE, colors = brewer.pal(8, "Spectral"))
ngramTokenizer <- function(l) {
function(x) unlist(lapply(ngrams(words(x), l), paste, collapse = " "), use.names = FALSE)
}
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_unigram <- generateNgramData(1)
final_bigram <- generateNgramData(2)
final_trigram <- generateNgramData(3)
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
final_ngram
}
final_unigram <- generateNgramData(1)
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_unigram <- generateNgramData(1)
final_bigram <- generateNgramData(2)
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines("en_US.blogs.txt",sep = "", skipNul = T))
ngramTokenizer <- function(l) {
function(x) unlist(lapply(ngrams(words(x), l), paste, collapse = ""), use.names = FALSE)
}
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = "", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_bigram <- generateNgramData(2)
final_trigram <- generateNgramData(3)
final_fourgram <- generateNgramData(4)
#Calculate probabilities
unigram_count <- sum(final_unigram$freq)
bigram_count <- sum(final_bigram$freq)
trigram_count <- sum(final_trigram$freq)
fourgram_count <- sum(final_fourgram$freq)
final_unigram <- transform(final_unigram, p = freq / unigram_count, pw = 0)
final_bigram <- transform(final_bigram, p = freq / bigram_count, pone = 0, termone = terms$one, termtwo = terms$two, terms = NULL)
final_trigram <- transform(final_trigram, p = freq / trigram_count, pw = 0, termone = terms$one, termtwo = terms$two, termthree = terms$three, terms = NULL)
final_fourgram <- transform(final_fourgram, p = freq / fourgram_count, pw = 0, termone = terms$one, termtwo = terms$two, termthree = terms$three, termfour = terms$four, terms = NULL)
#Significantly reduce data size by only keeping grams greater than the avg count
final_bigram_sm <- final_bigram[final_bigram$freq > mean(final_bigram$freq),]
final_trigram_sm <- final_trigram[final_trigram$freq > mean(final_trigram$freq),]
final_fourgram_sm <- final_fourgram[final_fourgram$freq > mean(final_fourgram$freq),]
bigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm.Rda")
trigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_trigram_sm.Rda")
fourgram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_fourgram_sm.Rda")
saveRDS(final_bigram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm_test.Rda")
saveRDS(final_trigram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm_test.Rda")
saveRDS(final_fourgram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm_test.Rda")
saveRDS(final_trigram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_trigram_sm_test.Rda")
saveRDS(final_fourgram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_fourgram_sm_test.Rda")
saveRDS(final_bigram_sm, file = "D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm_test.Rda")
bigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm.Rda")
trigram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_trigram_sm.Rda")
fourgram <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_fourgram_sm.Rda")
bigram_test <-readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_bigram_sm_test.Rda")
trigram_test <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_trigram_sm_test.Rda")
fourgram_test <- readRDS(file="D:/Coursera/Coursera-SwiftKey/final/en_US/final_fourgram_sm_test.Rda")
shiny::runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
find_rtools()
install.packages("devtools")
library(devtools)
find_rtools()
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
runApp('predictive-word-app')
getwd()
setwd("D:/Coursera/Coursera-SwiftKey/final/en_US/predictive-word-app")
runApp()
getwd()
runApp()
runApp()
runApp()
