title: \\\"Data Science Capstone Peer-graded Assignment: Milestone Report\\\"
author: \\\"Sarah M Wehkamp\\\"
date: \\\"April 25, 2025\\\"
output: html_document
---

## Introduction

The goal of this capstone project is to develop a predictive text model using a substantial text corpus as training data. Natural language processing (NLP) methodologies will be employed for this analysis.

## Loading (Blogs, News, Twitter)
\`\`\`{r}
# Set working directory 
setwd(\\\"D:/Coursera/Coursera-SwiftKey/final/en_US\\\")

# Load the data files
blogs <- readLines(\\\"en_US.blogs.txt\\\", warn = FALSE, encoding = \\\"UTF-8\\\")
news <- readLines(\\\"en_US.news.txt\\\", warn = FALSE, encoding = \\\"UTF-8\\\")
twitter <- readLines(\\\"en_US.twitter.txt\\\", warn = FALSE, encoding = \\\"UTF-8\\\")
\`\`\`

## Sampling
\`\`\`{r, echo=TRUE}
library(stringi) # stats files

# Calculate file sizes in MB
size_blogs <- file.info(\\\"en_US.blogs.txt\\\")\$size / 1024^2
size_news <- file.info(\\\"en_US.news.txt\\\")\$size  / 1024^2
size_twitter <- file.info(\\\"en_US.twitter.txt\\\")\$size / 1024^2

# Count the number of lines in each file
len_blogs <- length(blogs)
len_news <- length(news)
len_twitter <- length(twitter)

# Count the number of characters in each file
nchar_blogs <- sum(nchar(blogs))
nchar_news <- sum(nchar(news))
nchar_twitter <- sum(nchar(twitter))

# Count the number of words in each file
nword_blogs <- sum(stri_count_words(blogs))
nword_news <- sum(stri_count_words(news))
nword_twitter <-sum(stri_count_words(twitter))

# Create a summary table
data.frame(
  file.name = c(\\\"blogs\\\", \\\"news\\\", \\\"twitter\\\"),
  files.size.MB = c(size_blogs, size_news, size_twitter),
  num.lines = c(len_blogs, len_news, len_twitter),
  num.character = c(nchar_blogs, nchar_news, nchar_twitter),
  num.words = c(nword_blogs, nword_news, nword_twitter)
)
\`\`\`

## Sampling
To manage the large dataset size for processing, a sample dataset was created, comprising 1% of the data from each of the three original files, after removing non-English characters.
\`\`\`{r}
# Set a seed for reproducibility
set.seed(12345)

# Remove non-English characters
blogs1 <- iconv(blogs, \\\"latin1\\\", \\\"ASCII\\\", sub = \\\"\\\")
news1 <- iconv(news, \\\"latin1\\\", \\\"ASCII\\\", sub = \\\"\\\")
twitter1 <- iconv(twitter, \\\"latin1\\\", \\\"ASCII\\\", sub = \\\"\\\")

# Sample 1% of the lines from each file
sample_data <- c(
  sample(blogs1, length(blogs1) * 0.01),
  sample(news1, length(news1) * 0.01),
  sample(twitter1, length(twitter1) * 0.01)
)
\`\`\`

Given the substantial size of the original datasets, the \`sample()\` function was used to extract a representative subset of 1% from each file.

## Clean and Build Corpus
\`\`\`{r, echo=TRUE}
library(tm) # Text mining
library(NLP)

# Create a corpus from the sampled data
corpus <- VCorpus(VectorSource(sample_data))

# Apply text transformations
corpus1 <- tm_map(corpus, removePunctuation)
corpus2 <- tm_map(corpus1, stripWhitespace)
corpus3 <- tm_map(corpus2, tolower) # Convert to lowercase
corpus4 <- tm_map(corpus3, removeNumbers)
corpus5 <- tm_map(corpus4, PlainTextDocument)

# Remove stop words in English (a, as, at, so, etc.)
corpus6 <- tm_map(corpus5, removeWords, stopwords(\\\"english\\\"))
\`\`\`

## Build N-Grams
In Natural Language Processing (NLP), an *n*-gram is a contiguous sequence of $n$ items from a given sequence of text or speech. Unigrams represent single words, bigrams are combinations of two words, and trigrams consist of three-word sequences.

The following R function, utilizing the RWeka package, was employed to extract 1-grams, 2-grams, and 3-grams from the processed text corpus.
\`\`\`{r, echo=TRUE}
library(RWeka) # tokenizer - create unigrams, bigrams, trigrams

# Define tokenizer functions for unigrams, bigrams, and trigrams
one <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
two <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
thr <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

# Create term-document matrices for each n-gram type
one_table <- TermDocumentMatrix(corpus6, control = list(tokenize = one))
two_table <- TermDocumentMatrix(corpus6, control = list(tokenize = two))
thr_table <- TermDocumentMatrix(corpus6, control = list(tokenize = thr))

# Find frequent terms for each n-gram type
one_corpus <- findFreqTerms(one_table, lowfreq = 1000)
two_corpus <- findFreqTerms(two_table, lowfreq = 80)
thr_corpus <- findFreqTerms(thr_table, lowfreq = 10)

# Calculate term frequencies and create data frames
one_corpus_num <- rowSums(as.matrix(one_table[one_corpus, ]))
one_corpus_table <- data.frame(Word = names(one_corpus_num), frequency = one_corpus_num)
one_corpus_sort <- one_corpus_table[order(-one_corpus_table\$frequency), ]
head(one_corpus_sort)

two_corpus_num <- rowSums(as.matrix(two_table[two_corpus, ]))
two_corpus_table <- data.frame(Word = names(two_corpus_num), frequency = two_corpus_num)
two_corpus_sort <- two_corpus_table[order(-two_corpus_table\$frequency), ]
head(two_corpus_sort)

thr_corpus_num <- rowSums(as.matrix(thr_table[thr_corpus, ]))
thr_corpus_table <- data.frame(Word = names(thr_corpus_num), frequency = thr_corpus_num)
thr_corpus_sort <- thr_corpus_table[order(-thr_corpus_table\$frequency), ]
head(thr_corpus_sort)
\`\`\`

## Exploratory Analysis (Graphs & Visualizations)
The frequency distribution of the top 10 most frequent unigrams, bigrams, and trigrams were visualized using bar plots.
\`\`\`{r, echo=TRUE}
library(ggplot2) #visualization

# Create bar plot for unigrams
one_g <- ggplot(one_corpus_sort[1:10, ], aes(x = reorder(Word, -frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = \\\"identity\\\") +
  labs(title = \\\"Unigrams - Top 10 Frequencies\\\", x = \\\"Words\\\", y = \\\"Frequency\\\") +
  theme(axis.text.x = element_text(angle = 90))
one_g

# Create bar plot for bigrams
two_g <- ggplot(two_corpus_sort[1:10, ], aes(x = reorder(Word, -frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = \\\"identity\\\") +
  labs(title = \\\"Bigrams - Top 10 Frequencies\\\", x = \\\"Words\\\", y = \\\"Frequency\\\") +
  theme(axis.text.x = element_text(angle = 90))
two_g

# Create bar plot for trigrams
thr_g <- ggplot(thr_corpus_sort[1:10, ], aes(x = reorder(Word, -frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = \\\"identity\\\") +
  labs(title = \\\"Trigrams - Top 10 Frequencies\\\", x = \\\"Words\\\", y = \\\"Frequency\\\") +
  theme(axis.text.x = element_text(angle = 90))
thr_g
\`\`\`

## Conclusion & Next Steps
A potential user interface for the Shiny application would involve a text input box.

Next Steps:
1. Develop the predictive algorithm.
2. Construct a Shiny web application that suggests the most probable next word following user input.
3. Prepare a presentation outlining the application and deploy it on the \\\"shinyapps.io\\\" server.
")
